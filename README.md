# wikipedia-bias
Luke Wiljanen, Luca Heines

[literature Review](literature-review.md)

## Abstract
This project explores how AI can be used to detect and analyze bias in Wikipedia content. Drawing from prior research on structural, gender, and representation biases, it examines how biased language appears, evolves, or is corrected over time. Using Wikipedia's text and edit histories, the study applies word embeddings, metadata analysis, and simple bias detection tools to trace patterns of bias across articles and topics. A key focus is on how AI-assisted editing impacts the persistence or reduction of these biases, and whether community revisions help mitigate or unintentionally reinforce them. This research aims to inform more equitable and transparent use of AI in collaborative knowledge platforms.


## Research Questions
<b>How does AI-assisted editing influence the introduction and persistence of bias in Wikipedia, and what patterns show whether community revisions reduce or reinforce these biases?</b>
<br>As AI tools become more integrated into Wikipedia editing, they have the potential to both reduce and introduce bias. While they can flag problematic content at scale, they may also replicate biases from their training data. This project examines how AI-assisted edits influence the presence of bias over time, and whether human revisions help correct or unintentionally reinforce those biases. Understanding these patterns is key to ensuring AI supports a more neutral and inclusive Wikipedia.


<b>Can AI models actually detect gender bias in Wikipedia articles?</b>
<br>Gender bias in Wikipedia biographies often shows up through unequal coverage, language, or framing of women compared to men. AI models offer a scalable way to detect these patterns, but they may also reflect the same biases present in their training data. This project explores how effectively AI can identify gender bias and what limitations exist when relying on automated tools for such a nuanced issue.



## Methodology



